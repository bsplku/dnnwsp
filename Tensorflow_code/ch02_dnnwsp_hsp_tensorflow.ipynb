{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight sparsity control (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for weight sparsity control in MLP.  \n",
    "It is written for Python 3.5/3.6 and Tensorflow 1.1.0.  \n",
    "We can easily implement weight sparsity control follwing several steps described below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, call modules which containing Python definitions and statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import scipy.io\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 01. Customization part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "â€» This part would be modified to GUI.  \n",
    "\n",
    "You can change the mode either for classification or autoencoder. => toggle switch  \n",
    "Also, can change weight sparsity control mode in layer wise or node wise. => toggle switch  \n",
    "You may select optimzer algorithm among those five. => five buttons  \n",
    "Default file loaded as dataset is our brain data. but you can load yours later. => file dialog  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "autoencoder or not\n",
    "\"\"\"\n",
    "autoencoder=False\n",
    "\n",
    "\"\"\"\n",
    "Select the sparsity control mode\n",
    "'layer' for layer wise sparsity control\n",
    "'node' for node wise sparsity control\n",
    "\"\"\"\n",
    "mode = 'node'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Select optimizer\n",
    "'Grad' for GradientDescentOptimizer\n",
    "'Ada' for AdagradOptimizer\n",
    "'Moment' for MomentumOptimizer\n",
    "'Adam' for AdamOptimizer\n",
    "'RMSP' for RMSPropOptimizer\n",
    "\"\"\"\n",
    "optimizer_algorithm='Grad'\n",
    "\n",
    "\"\"\"\n",
    "Load your own data here\n",
    "\"\"\"\n",
    "dataset = scipy.io.loadmat('/home/hailey/01_study/prni2017_samples/lhrhadvs_sample_data2.mat')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set the number of nodes the value of first element is for input layer, the last one for output layer,  and the others in the minddle for hidden layers. => input dialog  \n",
    "We can adjust learning parameters total epoch, mini-batch size, when to begin learning rate annealing, decaying rate of learning rate, initial value of learning rate, and minimum value of learning rate. Besides, learning rate of beta of weight sparsity control(for L1 regularization) and L2 parameter(for L2 regularization). =>  input dialog  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "Set the number of nodes for input, output and each hidden layer here\n",
    "\"\"\"\n",
    "nodes=[74484,100,100,100,4]\n",
    "\n",
    "\"\"\"\n",
    "Set learning parameters\n",
    "\"\"\"\n",
    "\n",
    "# Set total epoch\n",
    "total_epoch=500\n",
    "# Set mini batch size\n",
    "batch_size=100\n",
    "# Let anealing to begin after **th epoch\n",
    "beginAnneal=200\n",
    "# anealing decay rate\n",
    "decay_rate=1e-4\n",
    "# Set initial learning rate and minimum                     \n",
    "lr_init = 1e-3    \n",
    "min_lr = 1e-4\n",
    "\n",
    "# Set learning rate of beta for weight sparsity control\n",
    "lr_beta = 0.015\n",
    "# Set L2 parameter for L2 regularization\n",
    "L2_param= 1e-5\n",
    "\n",
    "\"\"\"\n",
    "Set maximum beta value of each hidden layer (usually 0.01~0.5) \n",
    "and set target sparsness value (0:dense~1:sparse)\n",
    "\"\"\"\n",
    "\n",
    "max_beta = [0.05, 0.75, 0.7]\n",
    "tg_hsp = [0.7, 0.7, 0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data we provides consists of train, test, and validation sets. This part splits dataset into each of them. \n",
    "Input dimension is 74484, and the number of output nodes for classification is four (Left hand clenching, right hand clenching, auditory and visual tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into traning input\n",
    "train_input = dataset['train_x']\n",
    "# Split the dataset into test input\n",
    "test_input = dataset['test_x']\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into traning output \n",
    "train_output = np.zeros((np.shape(dataset['train_y'])[0],np.max(dataset['train_y'])+1))\n",
    "# trainsform classes into One-hot\n",
    "for i in np.arange(np.shape(dataset['train_y'])[0]):\n",
    "    train_output[i][dataset['train_y'][i][0]]=1 \n",
    "dataset['train_y']\n",
    "\n",
    "# Split the dataset into test output\n",
    "test_output = np.zeros((np.shape(dataset['test_y'])[0],np.max(dataset['test_y'])+1))\n",
    "# trainsform classes into One-hot\n",
    "for i in np.arange(np.shape(dataset['test_y'])[0]):\n",
    "    test_output[i][dataset['test_y'][i][0]]=1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part builds the MLP model by concatenating all layers based on the information we provided earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# We need 'node_index' for split placeholder (hidden_nodes=[100, 100, 100] -> nodes_index=[0, 100, 200, 300])\n",
    "nodes_index= [int(np.sum(nodes[1:i+1])) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Make placeholders to make our model in advance, then fill the values later when training or testing\n",
    "X=tf.placeholder(tf.float32,[None,nodes[0]])\n",
    "Y=tf.placeholder(tf.float32,[None,nodes[-1]])\n",
    "\n",
    "# Make weight variables which are randomly initialized\n",
    "w_init=[tf.div(tf.random_normal([nodes[i],nodes[i+1]]), tf.sqrt(float(nodes[i])/2)) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "w=[tf.Variable(w_init[i], dtype=tf.float32) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "# Make bias variables which are randomly initialized\n",
    "b=[tf.Variable(tf.random_normal([nodes[i+1]])) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Finally build our DNN model \n",
    "hidden_layers=[0.0]*(np.shape(nodes)[0]-2)\n",
    "for i in np.arange(np.shape(nodes)[0]-2):\n",
    "    \n",
    "    # Input layer\n",
    "    if i==0:\n",
    "        hidden_layers[i]=tf.add(tf.matmul(X,w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "        \n",
    "    # The other layers    \n",
    "    else:     \n",
    "        hidden_layers[i]=tf.add(tf.matmul(hidden_layers[i-1],w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "\n",
    "output_layer=tf.add(tf.matmul(hidden_layers[-1],w[-1]),b[-1])\n",
    "logRegression_layer=tf.nn.tanh(output_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of functions to create\n",
    "- beta vector\n",
    "- L1 loss\n",
    "- cost\n",
    "- optimzer\n",
    "- sparsity control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make placeholders for total beta vectors (make a long one to concatenate every beta vector) \n",
    "def build_betavec():\n",
    "    if mode=='layer':\n",
    "        Beta_vec=tf.placeholder(tf.float32,[np.shape(nodes)[0]-2])\n",
    "    elif mode=='node':\n",
    "        Beta_vec=tf.placeholder(tf.float32,[np.sum(nodes[1:-1])])\n",
    "\n",
    "    return Beta_vec\n",
    "\n",
    "\n",
    "# Make L1 loss term and L2 loss term for regularisation\n",
    "def build_L1loss():\n",
    "    if mode=='layer':\n",
    "        L1_loss=[Beta_vec[i]*tf.reduce_sum(abs(w[i])) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "#        L1_loss=[Beta_vec[i]*tf.reduce_mean(abs(w[i])) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "    elif mode=='node':\n",
    "        L1_loss=[tf.reduce_sum(tf.matmul(abs(w[i]),tf.cast(tf.diag(Beta_vec[nodes_index[i]:nodes_index[i+1]]),tf.float32))) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "#        L1_loss=[tf.reduce_mean(tf.matmul(abs(w[i]),tf.cast(tf.diag(Beta_vec[nodes_index[i]:nodes_index[i+1]]),tf.float32))) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "\n",
    "    return L1_loss\n",
    "\n",
    "       \n",
    "\n",
    "# Define cost term with cross entropy and L1 and L2 tetm     \n",
    "def build_cost():\n",
    "    if autoencoder:\n",
    "        # autoencoder uses mean squared error for the cost\n",
    "        cost=tf.reduce_mean(tf.pow(X - output_layer, 2)) + tf.reduce_sum(L1_loss) + L2_param*tf.reduce_sum(L2_loss)\n",
    "    else:\n",
    "        # the cross entropy of the result after applying the softmax function\n",
    "        cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logRegression_layer, labels=Y)) \\\n",
    "                                         + tf.reduce_sum(L1_loss) + L2_param*tf.reduce_sum(L2_loss)                                         \n",
    "    return cost\n",
    "\n",
    "\n",
    "# Define optimizer algorithm\n",
    "def build_optimizer(Lr):\n",
    "    if optimizer_algorithm=='Grad':\n",
    "        optimizer=tf.train.GradientDescentOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Ada':\n",
    "        optimizer=tf.train.AdagradOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Adam':\n",
    "        optimizer=tf.train.AdamOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Moment':\n",
    "        optimizer=tf.train.MomentumOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='RMSP':\n",
    "        optimizer=tf.train.RMSPropOptimizer(Lr).minimize(cost) \n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "if mode=='layer':\n",
    "    # Weight sparsity control with Hoyer's sparsness (Layer wise)  \n",
    "    def Hoyers_sparsity_control(w_,b,max_b,tg):\n",
    "        \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape  \n",
    "        num_elements=nodes*dim\n",
    " \n",
    "        Wvec=W.flatten()\n",
    "        \n",
    "        # Calculate L1 and L2 norm     \n",
    "        L1=LA.norm(Wvec,1)\n",
    "        L2=LA.norm(Wvec,2)\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h=(np.sqrt(num_elements)-(L1/L2))/(np.sqrt(num_elements)-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b-=lr_beta*np.sign(h-tg)\n",
    "        \n",
    "        # Trim value\n",
    "        b=0.0 if b<0.0 else b\n",
    "        b=max_b if b>max_b else b\n",
    "                         \n",
    "        return [h,b]\n",
    "    \n",
    "    \n",
    "elif mode=='node':   \n",
    "    # Weight sparsity control with Hoyer's sparsness (Node wise)\n",
    "    def Hoyers_sparsity_control(w_,b_vec,max_b,tg):\n",
    "    \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape\n",
    "        \n",
    "        # Calculate L1 and L2 norm \n",
    "        L1=LA.norm(W,1,axis=0)\n",
    "        L2=LA.norm(W,2,axis=0)\n",
    "        \n",
    "        h_vec = np.zeros((1,dim))\n",
    "        tg_vec = np.ones(dim)*tg\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h_vec=(np.sqrt(nodes)-(L1/L2))/(np.sqrt(nodes)-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b_vec-=lr_beta*np.sign(h_vec-tg_vec)\n",
    "        \n",
    "        # Trim value\n",
    "        b_vec[b_vec<0.0]=0.0\n",
    "        b_vec[b_vec>max_b]=max_b\n",
    "        \n",
    "               \n",
    "        return [h_vec,b_vec]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create them by using those functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = lr_init\n",
    "\n",
    "Beta_vec = build_betavec()\n",
    "\n",
    "L1_loss = build_L1loss()\n",
    "L2_loss = [tf.reduce_sum(tf.square(w[i])) for i in np.arange(np.shape(nodes)[0]-1)] \n",
    "#L2_loss = [tf.reduce_mean(tf.square(w[i])) for i in np.arange(np.shape(nodes)[0]-1)] \n",
    "\n",
    "cost = build_cost()\n",
    "\n",
    "\n",
    "# Make learning rate as placeholder to update learning rate every iterarion \n",
    "Lr=tf.placeholder(tf.float32)\n",
    "optimizer=build_optimizer(Lr)\n",
    "  \n",
    "\n",
    "if not autoencoder:\n",
    "    correct_prediction=tf.equal(tf.argmax(output_layer,1),tf.argmax(Y,1))  \n",
    "        \n",
    "    # calculate mean error(accuracy) depending on the frequency it predicts correctly   \n",
    "    error=1-tf.reduce_mean(tf.cast(correct_prediction,tf.float32))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Condition check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the conditions to catch some possible errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "condition=False\n",
    "\n",
    "print()\n",
    "\n",
    "if np.shape(nodes)[0] <3:\n",
    "    print(\"Error : Not enough hidden layer number.\")\n",
    "elif (autoencoder==False) & (np.shape(train_input)[0] != np.shape(train_output)[0]):\n",
    "    print(\"Error : The sizes of input train dataset and output train dataset don't match. \")  \n",
    "elif (autoencoder==False) & (np.shape(test_input)[0] != np.shape(test_output)[0]):\n",
    "    print(\"Error : The sizes of input test dataset and output test dataset don't match. \")     \n",
    "elif not ((mode=='layer') | (mode=='node')):\n",
    "    print(\"Error : Select a valid mode. \") \n",
    "elif (np.any(np.array(tg_hsp)<0)) | (np.any(np.array(tg_hsp)>1)):  \n",
    "    print(\"Error : Please set the target sparsities appropriately.\")\n",
    "elif (autoencoder!=False) & (autoencoder!=True):\n",
    "    print(\"Error : Please set the autoencoder mode appropriately.\")\n",
    "elif (autoencoder==True) & (np.shape(nodes)[0]>3) & (not all(x == nodes[::2][0] for x in nodes[::2])):\n",
    "    print(\"Error : The number of nodes in autoencoder is wrong.\")\n",
    "else:\n",
    "    condition=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Training & test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If condition is satisfied, may start session where training and tesing proceed.\n",
    "Firstly, initialize variables with initialization function.\n",
    "Then start learning , getting cost and optimizing, for all epochs. In every epoch, training data is split into mini batches so that every learning iteration is mini batch learning.\n",
    "At the end of every epoch, get training error and test error. Also, save cost, learning rate, beta, hsp and so on in order to plot them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if condition==True:\n",
    "    \n",
    "    # make initializer        \n",
    "    init = tf.global_variables_initializer()              \n",
    "\n",
    "    \n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "        \n",
    "        # run tensorflow variable initialization\n",
    "        sess.run(init)\n",
    "    \n",
    "        # initialization    \n",
    "        def initialization():           \n",
    "            if mode=='layer': \n",
    "                beta=np.zeros(np.shape(nodes)[0]-2)\n",
    "                beta_vec = np.zeros(np.shape(nodes)[0]-2)\n",
    "                hsp = np.zeros(np.shape(nodes)[0]-2)            \n",
    "                plot_beta = np.zeros(np.shape(nodes)[0]-2)\n",
    "                plot_hsp = np.zeros(np.shape(nodes)[0]-2)\n",
    "                           \n",
    "            elif mode=='node':                       \n",
    "                beta = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]  \n",
    "                beta_vec=np.zeros(np.sum(nodes[1:-1]))\n",
    "                hsp = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]            \n",
    "                plot_beta = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_hsp = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                \n",
    "            # make arrays to plot results\n",
    "            plot_lr=np.zeros(1)\n",
    "            plot_cost=np.zeros(1)\n",
    "            plot_train_err=np.zeros(1)\n",
    "            plot_test_err=np.zeros(1)\n",
    "            \n",
    "            return beta, beta_vec, hsp, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err\n",
    "                \n",
    "        beta, beta_vec, hsp, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err = initialization()\n",
    "        \n",
    "               \n",
    "\n",
    "        \n",
    "           \n",
    "       # Calculate how many mini-batch iterations\n",
    "        total_batch=int(np.shape(train_input)[0]/batch_size) \n",
    "               \n",
    "        # train and get cost\n",
    "        cost_avg=0.0\n",
    "        for epoch in np.arange(total_epoch):            \n",
    "            cost_epoch=0.0\n",
    "            \n",
    "            # Begin Annealing\n",
    "            if beginAnneal == 0:\n",
    "                lr = lr * 1.0\n",
    "            elif epoch+1 > beginAnneal:\n",
    "                lr = max( min_lr, (-decay_rate*(epoch+1) + (1+decay_rate*beginAnneal)) * lr )  \n",
    "            \n",
    "            # shuffle data in every epoch           \n",
    "            total_sample = np.size(train_input, axis=0)\n",
    "            sample_ids = np.arange(total_sample)\n",
    "            np.random.shuffle(sample_ids) \n",
    "\n",
    "        \n",
    "            # Train at each mini batch    \n",
    "            for batch in np.arange(total_batch):\n",
    "                batch_x = train_input[sample_ids[batch*batch_size:(batch+1)*batch_size]]\n",
    "                batch_y = train_output[sample_ids[batch*batch_size:(batch+1)*batch_size]]\n",
    "                \n",
    "                # Get cost and optimize the model\n",
    "                if autoencoder==True:\n",
    "                    cost_batch,_=sess.run([cost,optimizer],feed_dict={Lr:lr, X:batch_x, Beta_vec:beta_vec })\n",
    "                else:                   \n",
    "                    cost_batch,_=sess.run([cost,optimizer],feed_dict={Lr:lr, X:batch_x, Y:batch_y, Beta_vec:beta_vec })\n",
    "                    \n",
    "                cost_epoch+=cost_batch/total_batch      \n",
    "        \n",
    "                \n",
    "                \n",
    "        \n",
    "                # run weight sparsity control function\n",
    "                for i in np.arange(np.shape(nodes)[0]-2):\n",
    "                    [hsp[i],beta[i]]=Hoyers_sparsity_control(w[i], beta[i], max_beta[i], tg_hsp[i])   \n",
    "                    \n",
    "                if mode=='layer':               \n",
    "                    beta_vec=beta                      \n",
    "                elif mode=='node':                              \n",
    "                    beta_vec=[item for sublist in beta for item in sublist]\n",
    "            \n",
    "            if autoencoder==False:\n",
    "                train_input_suff = np.array([ train_input[i] for i in sample_ids])\n",
    "                train_output_suff = np.array([ train_output[i] for i in sample_ids])\n",
    "                \n",
    "                train_err_epoch=sess.run(error,feed_dict={X:train_input_suff, Y:train_output_suff})\n",
    "                plot_train_err=np.hstack([plot_train_err,[train_err_epoch]])\n",
    "                \n",
    "                test_err_epoch=sess.run(error,feed_dict={X:test_input, Y:test_output})\n",
    "                plot_test_err=np.hstack([plot_test_err,[test_err_epoch]])\n",
    "            \n",
    "            \n",
    "            # make space to plot beta, sparsity level\n",
    "            plot_lr=np.hstack([plot_lr,[lr]])\n",
    "            plot_cost=np.hstack([plot_cost,[cost_epoch]])\n",
    "\n",
    "            \n",
    "            # save footprint for plot\n",
    "            if mode=='layer':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[hsp[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[beta_vec[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                \n",
    "            elif mode=='node':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[np.transpose(hsp[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[np.transpose(beta[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "            \n",
    "                    \n",
    "            # Print cost at each epoch        \n",
    "            print(\"< Epoch\", \"{:02d}\".format(epoch+1),\"> Cost : \", \"{:.4f}\".format(cost_epoch))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Print final accuracy of test set\n",
    "        if autoencoder==False:\n",
    "            print(\"Accuracy :\",1-sess.run(error,feed_dict={X:test_input, Y:test_output}))\n",
    "            \n",
    "else:\n",
    "    # Don't run the sesstion but print 'failed' if any condition is unmet\n",
    "    print(\"Failed!\")  \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Plot & save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, show the the results and save them as .mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if condition==True:\n",
    "       \n",
    "    # Plot the change of learning rate\n",
    "    plt.title(\"Learning rate plot\",fontsize=16)\n",
    "    plot_lr=plot_lr[1:]\n",
    "    plt.ylim(0.0, lr_init*1.2)\n",
    "    plt.plot(plot_lr)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the change of cost\n",
    "    plt.title(\"Cost plot\",fontsize=16)\n",
    "    plot_cost=plot_cost[1:]\n",
    "    plt.plot(plot_cost)\n",
    "    plt.show()   \n",
    "    \n",
    "#    # Plot the change of cost\n",
    "#    plt.title(\"Cost plot (log scale)\",fontsize=16)\n",
    "#    plt.plot(plot_cost)\n",
    "#    plt.yscale('log')\n",
    "#    plt.show()     \n",
    "#    \n",
    "#    \n",
    "    if autoencoder==False:       \n",
    "        # Plot test error\n",
    "        plt.title(\"Training & Test error\",fontsize=16)\n",
    "        plot_test_err=plot_test_err[1:]\n",
    "        plt.plot(plot_train_err)\n",
    "        plt.hold\n",
    "        plt.plot(plot_test_err)\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.legend(['Training error', 'Test error'],loc='upper right')\n",
    "    #    plt.yscale('log')\n",
    "        plt.show() \n",
    "    \n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    # Plot the change of beta value\n",
    "    print(\"\")       \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        print(\"                  < Hidden layer\",i+1,\">\")\n",
    "        plt.title(\"Beta plot\",fontsize=16)\n",
    "        plot_beta[i]=plot_beta[i][1:]\n",
    "        plt.plot(plot_beta[i])\n",
    "        plt.ylim(0.0, np.max(max_beta)*1.2)\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot the change of Hoyer's sparsness\n",
    "    print(\"\")            \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        print(\"                  < Hidden layer\",i+1,\">\")\n",
    "        plt.title(\"Hoyer's sparsness plot\",fontsize=16)\n",
    "        plot_hsp[i]=plot_hsp[i][1:]\n",
    "        plt.plot(plot_hsp[i])\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.show()\n",
    "    \n",
    "    # make a new 'results' directory in the current directory\n",
    "    current_directory = os.getcwd()\n",
    "    final_directory = os.path.join(current_directory, r'results')\n",
    "    if not os.path.exists(final_directory):\n",
    "        os.makedirs(final_directory) \n",
    "        \n",
    "    # save results as .mat file\n",
    "    scipy.io.savemat(\"results/result_learningrate.mat\", mdict={'lr': plot_lr})\n",
    "    scipy.io.savemat(\"results/result_cost.mat\", mdict={'cost': plot_cost})\n",
    "    scipy.io.savemat(\"results/result_train_err.mat\", mdict={'trainErr': plot_train_err})\n",
    "    scipy.io.savemat(\"results/result_test_err.mat\", mdict={'testErr': plot_test_err})\n",
    "    scipy.io.savemat(\"results/result_beta.mat\", mdict={'beta': plot_beta})\n",
    "    scipy.io.savemat(\"results/result_hsp.mat\", mdict={'hsp': plot_hsp})\n",
    "\n",
    "else:\n",
    "    None "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
