{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion prediction using deep neural network (DNN) with weight sparsity control scheme\n",
    "\n",
    "An artificial neural network with multiple hidden layers (known as a deep neural network, or DNN) was employed as a predictive model (DNNp) to predict emotional responses using whole-brain functional magnetic resonance imaging (fMRI) data from individual subjects. During fMRI data acquisition, 10 healthy participants listened to 80 International Affective Digital Sound stimuli and rated their own emotions generated by each sound stimulus in terms of the arousal, dominance, and valence dimensions. The whole-brain spatial patterns from a general linear model (i.e., beta-valued maps) for each sound stimulus and the emotional response ratings were used as the input and output for the DNNP, respectively.\n",
    "\n",
    "This code is for the DNN based regression analysis and will focus on a three-hidden-layer DNN. We start off by implementing the code with Python 3.5/3.6 and Theano with the following steps:\n",
    "\n",
    "00. Import\n",
    "  - To use packages, libraries or modules provided in Python or Theano\n",
    "01. Function definition \n",
    "  - \"hsp_fnc_inv_mat_cal\": Estimation of weight sparsity level \n",
    "  - \"get_corrupted_input\": Corrupting input data\n",
    "02. Class Definition\n",
    "  - \"LinearRegression\": linear activation function at the output layer\n",
    "  - \"Hidden layer\": Initializing weight and bias parameters\n",
    "  - \"MLP\": Defining an architecture of your DNN model (here, elastic net scheme; three hidden layers; input corruption scheme)\n",
    "03. Parameters\n",
    "  - Set up the initial parameters: the number of the input nodes and hidden layers/nodes, learning rate, the number of epochs to train the DNN model\n",
    "04. Input data\n",
    "  - load input data and separation of the train and test set\n",
    "05. Build Model\n",
    "  - Define the cost function to train the DNN model\n",
    "06. Learning model\n",
    "  - Iteration of learning \n",
    "07. Save variables\n",
    "  - Store important variables such as trained weights/biases, predicted responses from the DNN model, and so on\n",
    "08. Main code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Import\n",
    "\n",
    "First, we need to import necessary modules/libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys # To print error or simple message\n",
    "import timeit  # To calculate computational time\n",
    "\n",
    "import numpy # NumPy is the fundamental package for scientific computing with Python.\n",
    "import numpy as np  # Simplification\n",
    "from numpy import linalg as LA \n",
    "\n",
    "import scipy.io as sio # The module for file input and output\n",
    "import scipy.stats # This module contains a large number of probability distributions as well as a growing library of statistical functions.\n",
    "\n",
    "import theano # Theano is the fundamental package for scientific computing with Python.\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "rng = numpy.random.RandomState(123)\n",
    "theano_rng = RandomStreams(rng.randint(2 ** 30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Function Definition\n",
    "\n",
    "We defined two functions to (1) estimate weight sparsity level of each node in each hidden layer and (2) corrupt the input data by zero-out randomly selected subset of size \"corruption_level\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the node-wise control of weight sparsity via Hoyer sparseness\n",
    "# (Hoyer, 2014, Kim and Lee PRNI2016, Kim and Lee ICASSP 2017)\n",
    "\n",
    "def hsp_fnc_inv_mat_cal(val_L1_ly, W, thre, tg, lrate):\n",
    "    W = np.array(W.get_value(borrow=True));\n",
    "    \n",
    "    [dim, nodes] = W.shape\n",
    "    \n",
    "    cnt_L1_ly = val_L1_ly;\n",
    "    \n",
    "    hsp_vec = np.zeros((1,nodes));  \n",
    "    \n",
    "    tg_vec = np.ones(nodes)*tg;\n",
    "    sqrt_nsamps = pow(dim,0.5)\n",
    "        \n",
    "    n1_W = LA.norm(W,1,axis=0);    n2_W = LA.norm(W,2,axis=0);\n",
    "    hsp_vec = (sqrt_nsamps - (n1_W/n2_W))/(sqrt_nsamps-1)\n",
    "    \n",
    "    cnt_L1_ly -= lrate*np.sign(hsp_vec-tg_vec)\n",
    "        \n",
    "    for ii in range(0,nodes):\n",
    "        if cnt_L1_ly[ii] < 0:\n",
    "            cnt_L1_ly[ii] = 0\n",
    "        if cnt_L1_ly[ii] > thre:\n",
    "            cnt_L1_ly[ii] = thre\n",
    "        \n",
    "    hspset =[hsp_vec, cnt_L1_ly]\n",
    "\n",
    "    return hspset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is for the corruption function. This function keeps ``1-corruption_level`` entries of the inputs the same and zero-out randomly selected subset of size ``corruption_level`` It is noted that the first argument is the input data to produce the shape(size) of random numbers and the second argument is the probability of success of any trial this will produce an array of 0s and 1s where 1 has a probability of 1 - ``corruption_level`` and 0 with ``corruption_level``. The binomial function return int64 data type by default.  int64 multiplicated by the input type(floatX) always return float64.  To keep all data in floatX when floatX is float32, we set the dtype of the binomial to floatX. As in our case the value of the binomial is always 0 or 1, this don't change the result. This is needed to allow the gpu to work correctly as it only support float32 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corrupted_input(input,corruption_level):\n",
    "        \n",
    "        return theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Class Definition\n",
    "\n",
    "Three classes for the linear regression, hidden layer, and multilayer perceptron were defined. Here, the linear activation function was defined for the output of the DNN output layer. You can simply change the output activation fucntion as the non-linear function from the code, \"T.dot(input, self.W) + self.b\" as \"T.tanh(T.dot(input, self.W) + self.b)\", and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(value=numpy.ones((n_in, n_out), \\\n",
    "                                                 dtype=theano.config.floatX), \\\n",
    "                                                    name='W', borrow=True)\n",
    "        # initialize the baises b as a vector of n_out 0s\n",
    "        self.b = theano.shared(value=numpy.ones((n_out,), \\\n",
    "                                                 dtype=theano.config.floatX), \\\n",
    "                                                    name='b', borrow=True)\n",
    "        self.p_y_given_x =  T.dot(input, self.W) + self.b\n",
    "        self.y_pred = self.p_y_given_x[:,0]\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial value of the weight parameters of a hidden layer should be uniformly distributed from a symmetric interval depending on the activation function. Here, we will use a hyperbolic activation function. Thus, based on the previous literature (Bengio and Glorot, 2010), the inverval should be defined as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
    "                 activation=T.tanh):\n",
    "        self.input = input\n",
    "    \n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == T.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the HiddenLayer class, we can implement the MLP class with a more efficient way as the following short implementation. In this tutorial, we will use L1 and L2 regularization, together and also use the input denoising scheme. For this, we need to compute the L1 norm and the squared L2 norm of the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, rng, input, n_in, n_hidden1, n_hidden2, n_hidden3, n_out,corruption_level,is_train):\n",
    "        \n",
    "        # Get corrupted input data \n",
    "        corrupted_x = get_corrupted_input(input,corruption_level)\n",
    "        # pseudo boolean for switching between training and prediction\n",
    "        input_x = T.switch(T.neq(is_train, 0), corrupted_x, input)\n",
    "        \n",
    "        self.hiddenLayer1 = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=input_x,\n",
    "            n_in=n_in,\n",
    "            n_out=n_hidden1,\n",
    "            activation=T.tanh\n",
    "        )\n",
    "        \n",
    "        self.hiddenLayer2 = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=self.hiddenLayer1.output,\n",
    "            n_in=n_hidden1,\n",
    "            n_out=n_hidden2,\n",
    "            activation=T.tanh\n",
    "        )\n",
    "        \n",
    "        self.hiddenLayer3 = HiddenLayer(\n",
    "            rng=rng,\n",
    "            input=self.hiddenLayer2.output,\n",
    "            n_in=n_hidden2,\n",
    "            n_out=n_hidden3,\n",
    "            activation=T.tanh\n",
    "        )\n",
    "         # The Linear regression layer gets as input the hidden units\n",
    "        # of the hidden layer\n",
    "        self.linearRegressionLayer = LinearRegression(\n",
    "            input=self.hiddenLayer3.output,\n",
    "            n_in=n_hidden3,\n",
    "            n_out=n_out\n",
    "        )\n",
    "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
    "        # be small\n",
    "        self.L1_layer1 = (\n",
    "            abs(self.hiddenLayer1.W).sum()\n",
    "        )\n",
    "        self.L1_layer2 = (\n",
    "            abs(self.hiddenLayer2.W).sum()\n",
    "        )\n",
    "        self.L1_layer3 = (\n",
    "            abs(self.hiddenLayer3.W).sum()\n",
    "        )\n",
    "        self.L2_sqr = (\n",
    "            (self.hiddenLayer1.W ** 2).sum()\n",
    "            + (self.hiddenLayer2.W ** 2).sum()\n",
    "            + (self.hiddenLayer3.W ** 2).sum()\n",
    "            + (self.linearRegressionLayer.W ** 2).sum()\n",
    "        )\n",
    "        \n",
    "        self.errors = self.linearRegressionLayer.errors\n",
    "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params + self.hiddenLayer3.params+ self.linearRegressionLayer.params\n",
    "        self.oldparams = [theano.shared(np.zeros(p.get_value(borrow=True).shape, dtype=theano.config.floatX)) for p in self.params]\n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Parameters\n",
    "\n",
    "We will focus on the node-wise weight sparsity scheme in this tutorial so we will need to specify target sparsity levels of each hidden layer. In addition, the architecture of the DNN model should be specified in this phase. Here, we defined that the number of the input nodes, hidden nodes, and the output node were 55417 (i.e., in-brain voxels) ,20, 20, 20, 1, respectively. We can adjust a learning rate, an entire epoch to train the DNN model, batch size, decaying rate of the learning rate, an initial value of learning rate. Besides, a learning rate of beta of weight sparsity control (for L1 regularization) and L2 regularization parameter(for L2 regularization) can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_mlp():\n",
    "    rootpath = '/root/sharedfolder/code/demo_18aug22'\n",
    "    save_path = '/root/sharedfolder/code/demo_18aug22'\n",
    "    \n",
    "    sav_name = '%s/rst_vlnc_predcition.mat' % (save_path)  # a directory to save dnnwsp result  \n",
    "\n",
    "    [n_in, n_hidden1, n_hidden2, n_hidden3, n_output] =[55417,20,20,20,1] # DNN structure\n",
    "    val_L2 = 1e-5;    # L2-norm parameter\n",
    "    itlrate = 0.0005;   # learning rate \n",
    "    batch_size = 2;   # batch size \n",
    "    momentum =0.01;   # momentum\n",
    "    n_epochs = 500;   # the total number of epoch\n",
    "    scal_ref = 10;    # the scale for the emotion response \n",
    "    dcay_rate = 0.99; # decay learning rate for the learning rate \n",
    "    corruption_level = 0.3 \n",
    "    # entries of the inputs the same and zero-out randomly selected subset of size corruption_level\n",
    "        \n",
    "    # Parameters for the node-wise control of weight sparsity\n",
    "    # If you have three hidden layer, the number of target Hoyer's sparseness should be same \n",
    "    hsp_level = [0.7, 0.5, 0.3];  # Target sparsity     \n",
    "    max_beta = [0.03,0.5,0.5];  # Maximum beta changes\n",
    "    beta_lrates = 1e-2;\n",
    "    \n",
    "    rng = np.random.RandomState(8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample dataset we provide consists of two data sets such as train and test data. In the train data, there are 64 beta-valued maps (64 x 55,417) and 64 subjective responses (64 x 1) of valence. In the test data, there are 16 beta-valued maps and subjective response of valence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    sbjinfo = sio.loadmat('%s/emt_valence_sample.mat' % rootpath) \n",
    "    \n",
    "    ############# emt_sample_data.mat #############\n",
    "    # train_x  = 64 volumes x 55417 voxels  \n",
    "    # train_x  = 64 volumes x 1 [valence scores for traing]\n",
    "    # test_x  = 16 volumes x 55417 voxels\n",
    "    # test_y  = 16 volumes x 1 [valence scores for test]\n",
    "    ############################################################\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "        \n",
    "    train_x = sbjinfo['train_x']; \n",
    "    train_y = np.asarray(sbjinfo['train_y'],'float32').flatten() / scal_ref ;\n",
    "    \n",
    "    test_x = sbjinfo['test_x'];\n",
    "    test_y =  np.asarray(sbjinfo['test_y'],'float32').flatten() / scal_ref ;\n",
    "    \n",
    "    n_train_set_x = scipy.stats.zscore(train_x,axis=1,ddof=1)\n",
    "    n_test_set_x = scipy.stats.zscore(test_x,axis=1,ddof=1)\n",
    "    \n",
    "    n_trvld_batches = int(train_x.shape[0] / batch_size)\n",
    "    n_test_batches = int(n_test_set_x.shape[0] / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is to build the DNN model by concatenating all layers and defining the cost/lose function to train the DNN model. Having covered the basic concepts, writing an MLP class becomes quite easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "      \n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "    y = T.fvectors('y')  # the emotion responses are presented as a 1D vector\n",
    "    is_train = T.iscalar('is_train') # pseudo boolean for switching between training and prediction\n",
    "    \n",
    "    L1p_ly1 = T.fvector()  # index to a [mini]batch\n",
    "    L1p_ly2 = T.fvector()\n",
    "    L1p_ly3 = T.fvector()\n",
    "    L2p_ly = T.fscalar()\n",
    "    lrate = T.fscalar()\n",
    "\n",
    "    [op_tg_L1_ly1, op_tg_L1_ly2, op_tg_L1_ly3]= hsp_level\n",
    "    [max_beta_ly1, max_beta_ly2, max_beta_ly3] = max_beta\n",
    "\n",
    "    print ('... optimal HSP!!')\n",
    "    print ('%1.1f-%1.1f-%1.1f' % (op_tg_L1_ly1,op_tg_L1_ly2,op_tg_L1_ly3))\n",
    "    \n",
    "    hsp_ly1 = 0; hsp_ly2 = 0; hsp_ly3 =0;     val_L1_ly1 = 0; val_L1_ly2 = 0; val_L1_ly3=0;\n",
    "    list_hsp_ly1 = np.zeros((n_epochs,1));     list_hsp_ly2 = np.zeros((n_epochs,1));    list_hsp_ly3 = np.zeros((n_epochs,1))\n",
    "    list_L1_ly1 = np.zeros((n_epochs,1));     list_L1_ly2 = np.zeros((n_epochs,1));    list_L1_ly3 = np.zeros((n_epochs,1))\n",
    "    list_tr_err = np.zeros((n_epochs,1));    list_ts_err = np.zeros((n_epochs,1)) \n",
    "    lrate_list =np.zeros((n_epochs,1));\n",
    "\n",
    "    train_set_x = theano.shared(np.asarray(n_train_set_x, dtype=theano.config.floatX))\n",
    "    train_set_y = T.cast(theano.shared(train_y,borrow=True),'float32')\n",
    "\n",
    "    test_set_x = theano.shared(np.asarray(n_test_set_x, dtype=theano.config.floatX)); \n",
    "    test_set_y = T.cast(theano.shared(test_y,borrow=True),'float32')\n",
    "    \n",
    "    lrate_val = itlrate\n",
    "    \n",
    "    # construct the MLP class\n",
    "    \n",
    "    classifier = MLP(\n",
    "            rng=rng,                            \n",
    "            input=x,                            \n",
    "            n_in= n_in,\n",
    "            n_hidden1=n_hidden1,                         \n",
    "            n_hidden2=n_hidden2,                           \n",
    "            n_hidden3=n_hidden3,\n",
    "            n_out=n_output,\n",
    "            corruption_level = corruption_level,\n",
    "            is_train=is_train,\n",
    "        )\n",
    "            \n",
    "    # cost function\n",
    "    cost = ((classifier.linearRegressionLayer.y_pred-y)**2).sum()\n",
    "    cost += (T.dot(abs(classifier.hiddenLayer1.W),L1p_ly1)).sum(); \n",
    "    cost += (T.dot(abs(classifier.hiddenLayer2.W),L1p_ly2)).sum();\n",
    "    cost += (T.dot(abs(classifier.hiddenLayer3.W),L1p_ly3)).sum();\n",
    "    cost += L2p_ly * classifier.L2_sqr\n",
    "    \n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "    \n",
    "    new_gparams =[];                                    \n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "    new_gparams = [i/float(batch_size) for i in gparams]\n",
    "        \n",
    "    updates = []\n",
    "    \n",
    "    for param, gparam, oldparam in zip(classifier.params, new_gparams, classifier.oldparams):\n",
    "        delta = lrate * gparam + momentum * oldparam\n",
    "        updates.append((param, param - delta))\n",
    "        updates.append((oldparam, delta))\n",
    "                       \n",
    "    trvld_model = theano.function(\n",
    "        inputs=[index, L1p_ly1, L1p_ly2, L1p_ly3, L2p_ly, lrate],\n",
    "\n",
    "        outputs=[classifier.errors(y), classifier.linearRegressionLayer.y_pred],\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size],\n",
    "            is_train: np.cast['int32'](1)\n",
    "\n",
    "        },\n",
    "        allow_input_downcast = True,\n",
    "        on_unused_input = 'ignore',\n",
    "    )\n",
    "\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=[classifier.errors(y), classifier.linearRegressionLayer.y_pred],\n",
    "        givens={\n",
    "                x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "                y: test_set_y[index * batch_size:(index + 1) * batch_size],\n",
    "                is_train: np.cast['int32'](0)\n",
    "            },\n",
    "       on_unused_input='ignore'\n",
    "\n",
    "    )\n",
    "    \n",
    "    list_trvld_err = np.zeros((n_epochs,1)); tst_err = numpy.zeros((n_epochs,1));\n",
    "    pct_trvld = np.zeros((n_epochs,n_trvld_batches*batch_size))\n",
    "    pct_tst = np.zeros((n_epochs,n_test_batches*batch_size))\n",
    "    \n",
    "    hsp_val_ly1 = np.zeros((n_epochs+1,n_hidden1));    hsp_val_ly2 = np.zeros((n_epochs+1,n_hidden2));   hsp_val_ly3 = np.zeros((n_epochs+1,n_hidden3));\n",
    "    L1_val_ly1 = np.zeros((n_epochs+1,n_hidden1));    L1_val_ly2 = np.zeros((n_epochs+1,n_hidden2));    L1_val_ly3 = np.zeros((n_epochs+1,n_hidden3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn the DNN model with the weight sparsity control by iteratively updating parameters of weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    print ('... Training & Test')\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "    \n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        trvld_score = np.zeros((n_trvld_batches,1));\n",
    "        tmp_trvld_pct =0;\n",
    "        \n",
    "        for minibatch_index in range(n_trvld_batches):\n",
    "            tmp_mat_ly1 = (L1_val_ly1[epoch-1,:]);            tmp_mat_ly2 = (L1_val_ly2[epoch-1,:]);            tmp_mat_ly3 = (L1_val_ly3[epoch-1,:]);\n",
    "            trvld_out = trvld_model(minibatch_index,tmp_mat_ly1,tmp_mat_ly2,tmp_mat_ly3,val_L2,lrate_val)\n",
    "            if minibatch_index ==0:\n",
    "                tmp_trvld_pct = trvld_out[1]\n",
    "            else:\n",
    "                tmp_trvld_pct = np.concatenate((tmp_trvld_pct,trvld_out[1]),axis=0)\n",
    "            \n",
    "            [hsp_val_ly1[epoch,:], L1_val_ly1[epoch,:]] = hsp_fnc_inv_mat_cal(L1_val_ly1[epoch-1,:],classifier.hiddenLayer1.W,max_beta_ly1,op_tg_L1_ly1,beta_lrates)\n",
    "            [hsp_val_ly2[epoch,:], L1_val_ly2[epoch,:]] = hsp_fnc_inv_mat_cal(L1_val_ly2[epoch-1,:],classifier.hiddenLayer2.W,max_beta_ly2,op_tg_L1_ly2,beta_lrates)\n",
    "            [hsp_val_ly3[epoch,:], L1_val_ly3[epoch,:]] = hsp_fnc_inv_mat_cal(L1_val_ly3[epoch-1,:],classifier.hiddenLayer3.W,max_beta_ly3,op_tg_L1_ly3,beta_lrates)\n",
    "            \n",
    "        trvld_score=0;\n",
    "        trvld_score = (np.mean(abs(tmp_trvld_pct-train_y[numpy.arange(0,len(tmp_trvld_pct))])))\n",
    "        list_trvld_err[epoch-1] = trvld_score * scal_ref\n",
    "        pct_trvld[epoch-1][:] = tmp_trvld_pct\n",
    "        \n",
    "        tmp_test_pct=0;\n",
    "        for i in range(n_test_batches):\n",
    "            test_out = test_model(i)\n",
    "            if i ==0:\n",
    "                tmp_test_pct = test_out[1]\n",
    "            else:\n",
    "                tmp_test_pct = np.concatenate((tmp_test_pct,test_out[1]),axis=0)\n",
    "                \n",
    "        test_score = 0;\n",
    "        test_score = (np.mean(abs(tmp_test_pct-test_y[numpy.arange(0,len(tmp_test_pct))])))\n",
    "        tst_err[epoch-1] = test_score * scal_ref\n",
    "        pct_tst[epoch-1][:] = tmp_test_pct\n",
    "                           \n",
    "        lrate_val *= dcay_rate    \n",
    "        lrate_list[epoch-1] = lrate_val                        \n",
    "                \n",
    "        print('#######')         \n",
    "        print('CP %.2f inv_hsp-lrate %6f, test epoch %i/%i, minibatch %i/%i, tr_err %f, test_err %f' %\n",
    "            (corruption_level, lrate_list[epoch-1],epoch,n_epochs, minibatch_index+1, n_trvld_batches,trvld_score * scal_ref, test_score * scal_ref))\n",
    "        print ((\"hsp_ly1= %.3f/%.3f, L1p_ly1= %.3f, hsp_ly2= %.3f/%.3f, L1p_ly2= %.3f, hsp_ly3= %.3f/%.3f, L1p_ly3= %.3f \")\n",
    "               % (np.mean(hsp_val_ly1[epoch-1,:]),op_tg_L1_ly1,np.mean(L1_val_ly1[epoch-1,:]),\n",
    "                  np.mean(hsp_val_ly2[epoch-1,:]),op_tg_L1_ly2,np.mean(L1_val_ly2[epoch-1,:]),\n",
    "                  np.mean(hsp_val_ly3[epoch-1,:]),op_tg_L1_ly3,np.mean(L1_val_ly3[epoch-1,:])))           \n",
    "        \n",
    "        list_ts_err[epoch-1] = test_score * scal_ref\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Save variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will save the result of the DNN model as \"mat\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    if not os.path.exists(sav_path):\n",
    "        os.makedirs(sav_path)\n",
    "        \n",
    "    end_time = timeit.default_timer()\n",
    "    cst_time = (end_time - start_time) / 60.\n",
    "        \n",
    "    sio.savemat(sav_name, {'w1': classifier.hiddenLayer1.W.get_value(borrow=True),'b1': classifier.hiddenLayer1.b.get_value(borrow=True),\n",
    "                       'w2': classifier.hiddenLayer2.W.get_value(borrow=True),'b2': classifier.hiddenLayer2.b.get_value(borrow=True),\n",
    "                       'w3': classifier.hiddenLayer3.W.get_value(borrow=True),'b3': classifier.hiddenLayer3.b.get_value(borrow=True),\n",
    "                       'w4': classifier.linearRegressionLayer.W.get_value(borrow=True),'b4': classifier.linearRegressionLayer.b.get_value(borrow=True),\n",
    "                       'pct_trvld':pct_trvld,'pct_tst':pct_tst,'trvld_err':list_trvld_err,'ts_err':list_ts_err,'L2_val':val_L2,\n",
    "                       'l1ly1':L1_val_ly1,'l1ly2':L1_val_ly2,'l1ly3':L1_val_ly3,'hsply1':hsp_val_ly1,'hsply2':hsp_val_ly2,'hsply3':hsp_val_ly3,\n",
    "                       'l_rate':lrate_list,'cst_time':cst_time,'epch':epoch,'max_beta':max_beta,'beta_lrates':beta_lrates,\n",
    "                        'test_y':test_y,'train_y':train_y,'mtum':momentum,'btch_size':batch_size,'opt_hsp':hsp_level,'cp_lev':corruption_level})\n",
    "   \n",
    "    print ('...done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_mlp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
