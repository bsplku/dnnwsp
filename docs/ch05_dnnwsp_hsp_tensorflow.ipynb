{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight sparsity control (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for weight sparsity control in MLP.  \n",
    "It is written for Python 3.5/3.6 and Tensorflow 1.1.0.  \n",
    "We can easily implement weight sparsity control follwing several steps described below.  \n",
    "\n",
    "1. Import\n",
    "Importing necessary modules, functions\n",
    "Functiona definition\n",
    "Hoyer sparseness, ReLU, RmsProp, adam\n",
    "Class definition\n",
    "Hidden layer, multiple layer perceptron\n",
    "Parameters of dnnwsp\n",
    "Epoches, learning rates ...\n",
    "Input data\n",
    "Train and test data set from sample data set\n",
    "Build Model\n",
    "Building training dnnwsp model with L1 and L2 regulariation term using trainig data and test model using test data\n",
    "Learning\n",
    "Training dnnwsp model\n",
    "Save variables\n",
    "Save important variables\n",
    "logistic_sgd.py\n",
    "Logistic regression is a probabilistic, linear classifier\n",
    "Main function\n",
    "Execution the dnnwsp code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, call modules which containing Python definitions and statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This import statement gives Python access to all of TensorFlow's classes, methods, and symbols. \n",
    "import tensorflow as tf\n",
    "# NumPy is the fundamental package for scientific computing with Python.\n",
    "import numpy as np\n",
    "# Linear algebra module for calculating L1 and L2 norm  \n",
    "from numpy import linalg as LA\n",
    "# To plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "# To check the directory when saving the results\n",
    "import os.path\n",
    "# The module for file input and output\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 01. Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can specify either using weight sparsity control mode in layer wise or node wise.\n",
    "Also, may select optimzer algorithm among five selections.\n",
    "Then set the number of nodes the value of first element is for input layer, the last one for output layer,  and the others in the minddle for hidden layers. \n",
    "We can adjust learning parameters total epoch, mini-batch size, when to begin learning rate annealing, decaying rate of learning rate, initial value of learning rate, and minimum value of learning rate. Besides, learning rate of beta of weight sparsity control(for L1 regularization) and L2 parameter(for L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from customizationGUI \\\n",
    "        import mode, optimizer_algorithm, nodes, total_epoch, batch_size,\\\n",
    "        beginAnneal, decay_rate, lr_init, min_lr,lr_beta, L2_param, max_beta, tg_hsp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset we provides consists of train, test, and validation parts. This part splits dataset and save them. \n",
    "Input dimension is 74484, and the number of output nodes for classification is four (Left-hand clecnhing (LH), right-hand clecnhing, auditory attention (AD), and visual stimulus (VS) tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################ lhrhadvs_sample_data.mat ##################\n",
    "# train_x  = 240 volumes x 74484 voxels  \n",
    "# train_x  = 240 volumes x 1 [0:left-hand clenching task, 1:right-hand clenching task, 2:auditory task, 3:visual task]\n",
    "# test_x  = 120 volumes x 74484 voxels\n",
    "# test_y  = 120 volumes x 1 [0:left-hand clenching task, 1:right-hand clenching task, 2:auditory task, 3:visual task]\n",
    "############################################################\n",
    "\n",
    "datasets = sio.loadmat('lhrhadvs_sample_data.mat')\n",
    "\n",
    "\n",
    "train_x = datasets['train_x']\n",
    "train_y = np.zeros((np.shape(datasets['train_y'])[0],np.max(datasets['train_y'])+1))\n",
    "# transform into One-hot\n",
    "for i in np.arange(np.shape(datasets['train_y'])[0]):\n",
    "    train_y[i][datasets['train_y'][i][0]]=1 \n",
    "\n",
    "\n",
    "test_x = datasets['test_x']\n",
    "test_y = np.zeros((np.shape(datasets['test_y'])[0],np.max(datasets['test_y'])+1))\n",
    "# transform into One-hot\n",
    "for i in np.arange(np.shape(datasets['test_y'])[0]):\n",
    "    test_y[i][datasets['test_y'][i][0]]=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part builds the MLP model by concatenating all layers based on the information we provided earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'node_index' to split placeholder, for an example, given hidden_nodes=[100, 100, 100], nodes_index=[0, 100, 200, 300]\n",
    "nodes_index= [int(np.sum(nodes[1:i+1])) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Make two placeholders to fill the values later when training or testing\n",
    "X=tf.placeholder(tf.float32,[None,nodes[0]])\n",
    "Y=tf.placeholder(tf.float32,[None,nodes[-1]])\n",
    "\n",
    "# Create randomly initialized weight variables \n",
    "w_init=[tf.div(tf.random_normal([nodes[i],nodes[i+1]]), tf.sqrt(float(nodes[i])/2)) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "w=[tf.Variable(w_init[i], dtype=tf.float32) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "# Create randomly initialized bias variables \n",
    "b=[tf.Variable(tf.random_normal([nodes[i+1]]), dtype=tf.float32) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Build MLP model \n",
    "hidden_layers=[0.0]*(np.shape(nodes)[0]-2)\n",
    "for i in np.arange(np.shape(nodes)[0]-2):\n",
    "    # Input layer\n",
    "    if i==0:\n",
    "        hidden_layers[i]=tf.add(tf.matmul(X,w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "    # The other layers    \n",
    "    else:     \n",
    "        hidden_layers[i]=tf.add(tf.matmul(hidden_layers[i-1],w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "# Output layer\n",
    "output_layer=tf.add(tf.matmul(hidden_layers[-1],w[-1]),b[-1])\n",
    "\n",
    "# Logistic regression layer\n",
    "logRegression_layer=tf.nn.tanh(output_layer)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of functions to create\n",
    "- beta\n",
    "- L1 loss\n",
    "- L2 loss\n",
    "- cost\n",
    "- optimzer\n",
    "- sparsity control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if mode=='layer':\n",
    "    # Weight sparsity control with Hoyer's sparsness (Layer wise)  \n",
    "    def Hoyers_sparsity_control(w_,b,max_b,tg):\n",
    "        \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape  \n",
    "        \n",
    "        # vectorize weight matrix \n",
    "        Wvec=W.flatten()     \n",
    "        sqrt_nsamps=np.sqrt(Wvec.shape[0])\n",
    "        \n",
    "        # Calculate L1 and L2 norm     \n",
    "        L1=LA.norm(Wvec,1)\n",
    "        L2=LA.norm(Wvec,2)\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h=(sqrt_nsamps-(L1/L2))/(sqrt_nsamps-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b-=lr_beta*np.sign(h-tg)\n",
    "        \n",
    "        # Trim value\n",
    "        b=0.0 if b<0.0 else b\n",
    "        b=max_b if b>max_b else b\n",
    "                         \n",
    "        return [h,b]\n",
    "    \n",
    "    \n",
    "elif mode=='node':   \n",
    "    # Weight sparsity control with Hoyer's sparsness (Node wise)\n",
    "    def Hoyers_sparsity_control(w_,b_vec,max_b,tg):\n",
    "    \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape\n",
    "        sqrt_nsamps=np.sqrt(nodes)\n",
    "        \n",
    "        # Calculate L1 and L2 norm \n",
    "        L1=LA.norm(W,1,axis=0)\n",
    "        L2=LA.norm(W,2,axis=0)\n",
    "        \n",
    "        h_vec = np.zeros((1,dim))\n",
    "        tg_vec = np.ones(dim)*tg\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h_vec=(sqrt_nsamps-(L1/L2))/(sqrt_nsamps-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b_vec-=lr_beta*np.sign(h_vec-tg_vec)\n",
    "        \n",
    "        # Trim value\n",
    "        b_vec[b_vec<0.0]=0.0\n",
    "        b_vec[b_vec>max_b]=max_b\n",
    "        \n",
    "               \n",
    "        return [h_vec,b_vec]\n",
    "    \n",
    "\n",
    "\n",
    "# Make placeholders for total beta array (make a long one to concatenate every beta vector) \n",
    "def init_beta():\n",
    "    if mode=='layer':\n",
    "        # The size is same with the number of layers\n",
    "        Beta=tf.placeholder(tf.float32,[np.shape(nodes)[0]-2])\n",
    "    elif mode=='node':\n",
    "        # The size is same with the number of nodes\n",
    "        Beta=tf.placeholder(tf.float32,[np.sum(nodes[1:-1])])\n",
    "\n",
    "    return Beta\n",
    "\n",
    "\n",
    "# Make L1 loss term for regularization\n",
    "def init_L1loss():\n",
    "    if mode=='layer':\n",
    "        # Get L1 loss term by simply multiplying beta(scalar value) and L1 norm of weight for each layer\n",
    "        L1_loss=[Beta[i]*tf.reduce_sum(abs(w[i])) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "    elif mode=='node':\n",
    "        # Get L1 loss term by multiplying beta(vector values as many as nodes) and L1 norm of weight for each layer\n",
    "        L1_loss=[tf.reduce_mean(tf.matmul(abs(w[i]),tf.cast(tf.diag(Beta[nodes_index[i]:nodes_index[i+1]]),tf.float32))) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "    return L1_loss\n",
    "\n",
    "\n",
    "# Make L2 loss term for regularization\n",
    "def init_L2loss():\n",
    "    L2_loss=[tf.reduce_sum(tf.square(w[i])) for i in np.arange(np.shape(nodes)[0]-1)] \n",
    "    return L2_loss\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "# Define cost term (Cost = cross entropy + L1 term + L2 term )    \n",
    "def init_cost():\n",
    "\n",
    "    # A softmax regression : it adds up the evidence of our input being in certain classes, and converts that evidence into probabilities.\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logRegression_layer, labels=Y)) \\\n",
    "                                     + tf.reduce_sum(L1_loss) + L2_reg*tf.reduce_sum(L2_loss)   \n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "# TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function.\n",
    "def init_optimizer(Lr):\n",
    "    if optimizer_algorithm=='GradientDescent':\n",
    "        optimizer=tf.train.GradientDescentOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Adagrad':\n",
    "        optimizer=tf.train.AdagradOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Adam':\n",
    "        optimizer=tf.train.AdamOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Momentum':\n",
    "        optimizer=tf.train.MomentumOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='RMSProp':\n",
    "        optimizer=tf.train.RMSPropOptimizer(Lr).minimize(cost) \n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "# initialization   \n",
    "def init_otherVars():           \n",
    "    if mode=='layer': \n",
    "        beta_val = np.zeros(np.shape(nodes)[0]-2)\n",
    "        beta = np.zeros(np.shape(nodes)[0]-2)\n",
    "        hsp_val = np.zeros(np.shape(nodes)[0]-2)            \n",
    "        plot_beta = np.zeros(np.shape(nodes)[0]-2)\n",
    "        plot_hsp = np.zeros(np.shape(nodes)[0]-2)\n",
    "                   \n",
    "    elif mode=='node':                       \n",
    "        beta_val = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]  \n",
    "        beta = np.zeros(np.sum(nodes[1:-1]))\n",
    "        hsp_val = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]            \n",
    "        plot_beta = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "        plot_hsp = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "    \n",
    "    # make arrays to store and plot results\n",
    "    plot_lr=np.zeros(1)\n",
    "    plot_cost=np.zeros(1)\n",
    "    plot_train_err=np.zeros(1)\n",
    "    plot_test_err=np.zeros(1)\n",
    "    \n",
    "    # initialize learning rate\n",
    "    lr = lr_init \n",
    "    \n",
    "    \n",
    "    return lr, beta_val, beta, hsp_val, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create them by using those functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make a placeholder for learning rate to be able to update learning rate (Learning rate decaying) \n",
    "Lr=tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "Beta = init_beta()\n",
    "L1_loss = init_L1loss()\n",
    "L2_loss = init_L2loss()\n",
    "cost = init_cost()\n",
    "\n",
    "optimizer=init_optimizer(Lr)\n",
    "\n",
    "correct_prediction=tf.equal(tf.argmax(logRegression_layer,1),tf.argmax(Y,1))  \n",
    "# calculate an average error depending on how frequent it classified correctly   \n",
    "error=1-tf.reduce_mean(tf.cast(correct_prediction,tf.float32))      \n",
    "\n",
    "\n",
    "lr, beta_val, beta, hsp_val, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err = init_otherVars()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Condition check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the conditions to catch some possible errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition=False\n",
    "\n",
    "print()\n",
    "\n",
    "if np.size(nodes) <3:\n",
    "    print(\"Error : The number of total layers is not enough.\")\n",
    "elif (np.size(nodes)-2) != np.size(max_beta):\n",
    "    print(\"Error : The number of hidden layers and max beta values don't match. \")\n",
    "elif (np.size(nodes)-2) != np.size(tg_hsp):\n",
    "    print(\"Error : The number of hidden layers and target sparsity values don't match.\")\n",
    "elif np.size(train_x,axis=0) != np.size(train_y,axis=0):\n",
    "    print(\"Error : The sizes of input train datasets and output train datasets don't match. \")  \n",
    "elif np.size(test_x,axis=0) != np.size(test_y,axis=0):\n",
    "    print(\"Error : The sizes of input test datasets and output test datasets don't match. \")     \n",
    "elif (np.any(np.array(tg_hsp)<0)) | (np.any(np.array(tg_hsp)>1)):  \n",
    "    print(\"Error : The values of target sparsities are inappropriate.\")\n",
    "else:\n",
    "    condition=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If condition is satisfied, may start session where training and tesing proceed.\n",
    "Firstly, initialize variables with initialization function.\n",
    "Then start learning , getting cost and optimizing, for all epochs. In every epoch, training data is split into mini batches so that every learning iteration is mini batch learning.\n",
    "At the end of every epoch, get training error and test error. Also, save cost, learning rate, beta, hsp and so on in order to plot them later. \n",
    "\n",
    "â€» No need for 'feed_dict=' on Tensorflow version 1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if condition==True:\n",
    "\n",
    "    # variables are not initialized when you call tf.Variable\n",
    "    # To initialize all the variables in a TensorFlow program, you must explicitly call a special operation         \n",
    "    init = tf.global_variables_initializer()              \n",
    "   \n",
    "    \n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:           \n",
    "    \n",
    "        # run tensorflow variable initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "\n",
    "        # Start training \n",
    "        for epoch in np.arange(total_epoch):            \n",
    "                   \n",
    "            # Shuffle training data when starting each epoch           \n",
    "            total_sample = np.size(train_x, axis=0)\n",
    "            sample_ids = np.arange(total_sample)\n",
    "            np.random.shuffle(sample_ids) \n",
    "            \n",
    "            train_x_shuff = np.array([ train_x[i] for i in sample_ids])\n",
    "            train_y_shuff = np.array([ train_y[i] for i in sample_ids])\n",
    "            \n",
    "            \n",
    "            # Begin Annealing\n",
    "            if beginAnneal == 0:\n",
    "                lr = lr * 1.0\n",
    "            elif epoch+1 > beginAnneal:\n",
    "                lr = max( min_lr, (-decay_rate*(epoch+1) + (1+decay_rate*beginAnneal)) * lr )  \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            # Calculate how many mini-batch iterations we need\n",
    "            total_batch = int(np.shape(train_x)[0]/batch_size) \n",
    "            \n",
    "            cost_epoch=0.0\n",
    "            \n",
    "            # minibatch based training  \n",
    "            for batch in np.arange(total_batch):\n",
    "                batch_x = train_x_shuff[sample_ids[batch*batch_size:(batch+1)*batch_size]]\n",
    "                batch_y = train_y_shuff[sample_ids[batch*batch_size:(batch+1)*batch_size]]\n",
    "                \n",
    "                # Get cost and optimize the model\n",
    "                cost_batch,_=sess.run([cost,optimizer],{Lr:lr, X:batch_x, Y:batch_y, Beta:beta})\n",
    "\n",
    "                cost_epoch+=cost_batch/total_batch      \n",
    "        \n",
    "        \n",
    "                # weight sparsity control    \n",
    "                if mode=='layer':                   \n",
    "                    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "                        [hsp_val[i], beta_val[i]] = Hoyers_sparsity_control(w[i], beta_val[i], max_beta[i], tg_hsp[i])   \n",
    "                    beta=beta_val                      \n",
    "\n",
    "                elif mode=='node':                             \n",
    "                    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "                        [hsp_val[i], beta_val[i]] = Hoyers_sparsity_control(w[i], beta_val[i], max_beta[i], tg_hsp[i])   \n",
    "                    # flatten beta_val (shape (3, 100) -> (300,))\n",
    "                    beta=[item for sublist in beta_val for item in sublist]\n",
    "               \n",
    "            # get train error\n",
    "            train_err_epoch=sess.run(error,{X:train_x_shuff, Y:train_y_shuff})\n",
    "            plot_train_err=np.hstack([plot_train_err,[train_err_epoch]])\n",
    "            \n",
    "            # get test error\n",
    "            test_err_epoch=sess.run(error,{X:test_x, Y:test_y})\n",
    "            plot_test_err=np.hstack([plot_test_err,[test_err_epoch]])\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Save the results to plot at the end\n",
    "            plot_lr=np.hstack([plot_lr,[lr]])\n",
    "            plot_cost=np.hstack([plot_cost,[cost_epoch]])\n",
    "            \n",
    "            if mode=='layer':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[hsp_val[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[beta[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                \n",
    "            elif mode=='node':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[np.transpose(hsp_val[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[np.transpose(beta_val[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "            \n",
    "            # Print cost and errors after every training epoch       \n",
    "            print(\"< Epoch\", \"{:02d}\".format(epoch+1),\"> Cost :\", \"{:.3f}\".format(cost_epoch)\\\n",
    "                                            ,\"/ Train err :\", \"{:.3f}\".format(train_err_epoch),\"/ Test err :\",\"{:.3f}\".format(test_err_epoch))\n",
    "\n",
    "\n",
    "        # Print final accuracy on test set\n",
    "        print (\"\")\n",
    "        print(\"* Test accuracy :\", \"{:.3f}\".format(1-sess.run(error,{X:test_x, Y:test_y})))\n",
    "            \n",
    "else:\n",
    "    # Don't run the session but print 'failed' if any condition is not met\n",
    "    print(\"Failed!\")  \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Plot & save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, show the the results and save them as .mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if condition==True:\n",
    "       \n",
    "    # Plot the change of learning rate\n",
    "    plt.title(\"Learning rate plot\",fontsize=16)\n",
    "    plot_lr=plot_lr[1:]\n",
    "    plt.ylim(0.0, lr_init*1.2)\n",
    "    plt.plot(plot_lr)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the change of cost\n",
    "    plt.title(\"Cost plot\",fontsize=16)\n",
    "    plot_cost=plot_cost[1:]\n",
    "    plt.plot(plot_cost)\n",
    "    plt.show()   \n",
    "    \n",
    " \n",
    "  \n",
    "    # Plot train & test error\n",
    "    plt.title(\"Training & Test error\",fontsize=16)\n",
    "    plot_train_err=plot_train_err[1:]\n",
    "    plt.plot(plot_train_err)\n",
    "    plt.hold\n",
    "    plot_test_err=plot_test_err[1:]\n",
    "    plt.plot(plot_test_err)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.legend(['Training error', 'Test error'],loc='upper right')\n",
    "    plt.show() \n",
    "\n",
    "\n",
    " \n",
    "    # Plot the change of beta value\n",
    "    print(\"\")       \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        print(\"                  < Hidden layer\",i+1,\">\")\n",
    "        plt.title(\"Beta plot\",fontsize=16)\n",
    "        plot_beta[i]=plot_beta[i][1:]\n",
    "        plt.plot(plot_beta[i])\n",
    "        plt.ylim(0.0, np.max(max_beta)*1.2)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # Plot the change of Hoyer's sparsity\n",
    "    print(\"\")            \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        print(\"                  < Hidden layer\",i+1,\">\")\n",
    "        plt.title(\"Hoyer's sparsity plot\",fontsize=16)\n",
    "        plot_hsp[i]=plot_hsp[i][1:]\n",
    "        plt.plot(plot_hsp[i])\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.show()\n",
    "    \n",
    "    # make a new 'results' directory in the current directory\n",
    "    current_directory = os.getcwd()\n",
    "    final_directory = os.path.join(current_directory, r'results')\n",
    "    if not os.path.exists(final_directory):\n",
    "        os.makedirs(final_directory) \n",
    "        \n",
    "    # save results as .mat file\n",
    "    sio.savemat(\"results/result_learningrate.mat\", mdict={'lr': plot_lr})\n",
    "    sio.savemat(\"results/result_cost.mat\", mdict={'cost': plot_cost})\n",
    "    sio.savemat(\"results/result_train_err.mat\", mdict={'trainErr': plot_train_err})\n",
    "    sio.savemat(\"results/result_test_err.mat\", mdict={'testErr': plot_test_err})\n",
    "    sio.savemat(\"results/result_beta.mat\", mdict={'beta': plot_beta})\n",
    "    sio.savemat(\"results/result_hsp.mat\", mdict={'hsp': plot_hsp})\n",
    "\n",
    "else:\n",
    "    None "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
