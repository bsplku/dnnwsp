{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to install Tensorflow (Linux, Mac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- This installation guide refers to 'Tensorflow website- tensorflow.org'\n",
    "\n",
    "\n",
    "\n",
    "There are several ways to install tensorflow : virtualenv / \"native\" pip / Docker / Anaconda\n",
    "\n",
    "But, as they recommend the virtualenv installation, \n",
    "becasue virtualenv is a virtual Python environment isolated from other Python development, incapable of interfering with or being affected by other Python programs on the same machine. \n",
    "During the virtualenv installation process, you will install not only TensorFlow but also all the packages that TensorFlow requires. (This is actually pretty easy.) \n",
    "To start working with TensorFlow, you simply need to \"activate\" the virtual environment. \n",
    "All in all, virtualenv provides a safe and reliable mechanism for installing and running TensorFlow.\n",
    "\n",
    "We will install 'CPU support only' version. If you are insterested in 'GPU support version', please try at home.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Install pip and virtualenv by issuing the following command:\n",
    " - Linux  \n",
    "> ＄ sudo apt-get install python-pip python-dev python-virtualenv\n",
    "\n",
    " - Mac  \n",
    "<cite> - Start a terminal (a shell). You'll perform all subsequent steps in this shell. </cite>\n",
    "> ＄ sudo easy_install pip  \n",
    "> ＄ sudo pip install --upgrade virtualenv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create a virtualenv environment by issuing the following command:  \n",
    "<cite> - The targetDirectory specifies the top of the virtualenv tree.  \n",
    "Our instructions assume that targetDirectory is ~/tensorflow, but you may choose any directory. </cite>\n",
    " - Linux  \n",
    "> ＄ virtualenv --system-site-packages targetDirectory\n",
    " - Mac  \n",
    "> ＄ virtualenv --system-site-packages -p python3 targetDirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3) Activate the virtualenv environment by issuing one of the following commands:  \n",
    " - Linux / Mac \n",
    "> ＄ source ~/tensorflow/bin/activate    # bash, sh, ksh, or zsh  \n",
    "\n",
    "or  \n",
    "  > ＄ source ~/tensorflow/bin/activate.csh     # csh or tcsh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Issue the following command to install TensorFlow in the active virtualenv environment:  \n",
    "<cite> The preceding source command should change your prompt to '(tensorflow)＄' </cite>  \n",
    " - Linux / Mac  \n",
    "> (tensorflow)＄ pip3 install --upgrade tensorflow   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cite> ※ If the preceding command fails, perform Step 5. </cite>  \n",
    "\n",
    "5) If Step 4 failed (typically because you invoked a pip version lower than 8.1), install TensorFlow in the active virtualenv environment by issuing a command of the following format:  \n",
    "> (tensorflow)＄ pip3 install --upgrade TF_PYTHON_URL \n",
    "\n",
    " - Linux  \n",
    "TF_PYTHON_URL for python 3.6 & CPU version is  \n",
    "https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.1.0-cp36-cp36m-linux_x86_64.whl  \n",
    " - Mac  \n",
    "TF_PYTHON_URL for python 3.4~6 & CPU version is  \n",
    "https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing TensorFlow, validate your installation to confirm that the installation worked properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. validate the installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  activate the virtualenv environment each time you use TensorFlow  \n",
    " - Linux / Mac\n",
    "> ＄ source ~/tensorflow/bin/activate      # bash, sh, ksh, or zsh  \n",
    "\n",
    "or  \n",
    "  > ＄ source ~/tensorflow/bin/activate.csh  # csh or tcsh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Your prompt will become the following to indicate that your tensorflow environment is active:  \n",
    " - Linux / Mac  \n",
    ">  (tensorflow)＄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) When you are done using TensorFlow, you may deactivate the environment  \n",
    " - Linux / Mac\n",
    "> (tensorflow)＄ deactivate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight sparsity control (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for weight sparsity control in MLP.\n",
    "It is written for Python 3.5/3.6 and Tensorflow 1.1.0 (An open-source software library for Machine Intelligence).\n",
    "We can easily implement weight sparsity control follwing several steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call modules, which are files containing Python definitions and statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import scipy.io\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 01. Customization part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-> This part would be modified to GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "autoencoder or not\n",
    "\"\"\"\n",
    "autoencoder=False\n",
    "\n",
    "\"\"\"\n",
    "Select the sparsity control mode\n",
    "'layer' for layer wise sparsity control\n",
    "'node' for node wise sparsity control\n",
    "\"\"\"\n",
    "mode = 'node'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Select optimizer\n",
    "'Grad' for GradientDescentOptimizer\n",
    "'Ada' for AdagradOptimizer\n",
    "'Moment' for MomentumOptimizer\n",
    "'Adam' for AdamOptimizer\n",
    "'RMSP' for RMSPropOptimizer\n",
    "\"\"\"\n",
    "optimizer_algorithm='Grad'\n",
    "\n",
    "\"\"\"\n",
    "Load your own data here\n",
    "\"\"\"\n",
    "dataset = scipy.io.loadmat('/home/hailey/01_study/prni2017_samples/lhrhadvs_sample_data2.mat')\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Set the number of nodes for input, output and each hidden layer here\n",
    "\"\"\"\n",
    "nodes=[74484,100,100,100,4]\n",
    "#nodes=[74484,500,74484]\n",
    "\n",
    "\"\"\"\n",
    "Set learning parameters\n",
    "\"\"\"\n",
    "# Set total epoch\n",
    "total_epoch=600\n",
    "# Set mini batch size\n",
    "batch_size=100\n",
    "# Let anealing to begin after 5th epoch\n",
    "beginAnneal=90\n",
    "# anealing decay rate\n",
    "decay_rate=1e-4\n",
    "# Set initial learning rate and minimum                     \n",
    "lr_init = 1e-3    \n",
    "min_lr = 1e-4\n",
    "\n",
    "# Set learning rate of beta for weight sparsity control\n",
    "lr_beta = 0.02\n",
    "# Set L2 parameter for L2 regularization\n",
    "L2_param= 1e-5\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Set maximum beta value of each hidden layer (usually 0.01~0.2) \n",
    "and set target sparsness value (0:dense~1:sparse)\n",
    "\"\"\"\n",
    "max_beta = [0.05, 0.8, 0.8]\n",
    "tg_hsp = [0.7, 0.65, 0.65]\n",
    "\n",
    "#max_beta = [0.02]\n",
    "#tg_hsp = [0.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Input data part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will let us load our data.\n",
    "Data provided here is neuroimage dataset which consists of train, test, and validation set.\n",
    "input layer would have 74484 nodes, and output layer 4 nodes for classification into 4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataset into traning input\n",
    "train_input = dataset['train_x']\n",
    "# Split the dataset into test input\n",
    "test_input = dataset['test_x']\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into traning output \n",
    "train_output = np.zeros((np.shape(dataset['train_y'])[0],np.max(dataset['train_y'])+1))\n",
    "# trainsform classes into One-hot\n",
    "for i in np.arange(np.shape(dataset['train_y'])[0]):\n",
    "    train_output[i][dataset['train_y'][i][0]]=1 \n",
    "dataset['train_y']\n",
    "\n",
    "# Split the dataset into test output\n",
    "test_output = np.zeros((np.shape(dataset['test_y'])[0],np.max(dataset['test_y'])+1))\n",
    "# trainsform classes into One-hot\n",
    "for i in np.arange(np.shape(dataset['test_y'])[0]):\n",
    "    test_output[i][dataset['test_y'][i][0]]=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Structure part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will build our MLP model by concatenating layers we've made before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# We need 'node_index' for split placeholder (hidden_nodes=[100, 100, 100] -> nodes_index=[0, 100, 200, 300])\n",
    "nodes_index= [int(np.sum(nodes[1:i+1])) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Make placeholders to make our model in advance, then fill the values later when training or testing\n",
    "X=tf.placeholder(tf.float32,[None,nodes[0]])\n",
    "Y=tf.placeholder(tf.float32,[None,nodes[-1]])\n",
    "\n",
    "# Make weight variables which are randomly initialized\n",
    "w_init=[tf.div(tf.random_normal([nodes[i],nodes[i+1]]), tf.sqrt(float(nodes[i])/2)) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "w=[tf.Variable(w_init[i], dtype=tf.float32) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "# Make bias variables which are randomly initialized\n",
    "b=[tf.Variable(tf.random_normal([nodes[i+1]])) for i in np.arange(np.shape(nodes)[0]-1)]\n",
    "\n",
    "# Finally build our DNN model \n",
    "hidden_layers=[0.0]*(np.shape(nodes)[0]-2)\n",
    "for i in np.arange(np.shape(nodes)[0]-2):\n",
    "    \n",
    "    # Input layer\n",
    "    if i==0:\n",
    "        hidden_layers[i]=tf.add(tf.matmul(X,w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "        \n",
    "    # The other layers    \n",
    "    else:     \n",
    "        hidden_layers[i]=tf.add(tf.matmul(hidden_layers[i-1],w[i]),b[i])\n",
    "        hidden_layers[i]=tf.nn.tanh(hidden_layers[i])\n",
    "\n",
    "output_layer=tf.add(tf.matmul(hidden_layers[-1],w[-1]),b[-1])\n",
    "logRegression_layer=tf.nn.tanh(output_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Learning part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of functions to create\n",
    "- beta_vec\n",
    "- L1 loss\n",
    "- cost\n",
    "- optimzer\n",
    "- sparsity control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make placeholders for total beta vectors (make a long one to concatenate every beta vector) \n",
    "def build_betavec():\n",
    "    if mode=='layer':\n",
    "        Beta_vec=tf.placeholder(tf.float32,[np.shape(nodes)[0]-2])\n",
    "    elif mode=='node':\n",
    "        Beta_vec=tf.placeholder(tf.float32,[np.sum(nodes[1:-1])])\n",
    "\n",
    "    return Beta_vec\n",
    "\n",
    "\n",
    "# Make L1 loss term and L2 loss term for regularisation\n",
    "def build_L1loss():\n",
    "    if mode=='layer':\n",
    "        L1_loss=[Beta_vec[i]*tf.reduce_sum(abs(w[i])) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "    elif mode=='node':\n",
    "        L1_loss=[tf.reduce_sum(tf.matmul(abs(w[i]),tf.cast(tf.diag(Beta_vec[nodes_index[i]:nodes_index[i+1]]),tf.float32))) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "    return L1_loss\n",
    "\n",
    "       \n",
    "\n",
    "# Define cost term with cross entropy and L1 and L2 tetm     \n",
    "def build_cost():\n",
    "    if autoencoder:\n",
    "        cost=tf.reduce_mean(tf.pow(X - output_layer, 2)) + tf.reduce_sum(L1_loss) + L2_param*tf.reduce_sum(L2_loss)\n",
    "    else:\n",
    "        cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logRegression_layer, labels=Y)) \\\n",
    "                                         + tf.reduce_sum(L1_loss) + L2_param*tf.reduce_sum(L2_loss)                                         \n",
    "    return cost\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "def build_optimizer(Lr):\n",
    "    if optimizer_algorithm=='Grad':\n",
    "        optimizer=tf.train.GradientDescentOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Ada':\n",
    "        optimizer=tf.train.AdagradOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Adam':\n",
    "        optimizer=tf.train.AdamOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='Moment':\n",
    "        optimizer=tf.train.MomentumOptimizer(Lr).minimize(cost) \n",
    "    elif optimizer_algorithm=='RMSP':\n",
    "        optimizer=tf.train.RMSPropOptimizer(Lr).minimize(cost) \n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "\n",
    "if mode=='layer':\n",
    "    # Weight sparsity control with Hoyer's sparsness (Layer wise)  \n",
    "    def Hoyers_sparsity_control(w_,b,max_b,tg):\n",
    "        \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape  \n",
    "        num_elements=nodes*dim\n",
    " \n",
    "        Wvec=W.flatten()\n",
    "        \n",
    "        # Calculate L1 and L2 norm     \n",
    "        L1=LA.norm(Wvec,1)\n",
    "        L2=LA.norm(Wvec,2)\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h=(np.sqrt(num_elements)-(L1/L2))/(np.sqrt(num_elements)-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b-=lr_beta*np.sign(h-tg)\n",
    "        \n",
    "        # Trim value\n",
    "        b=0.0 if b<0.0 else b\n",
    "        b=max_b if b>max_b else b\n",
    "                         \n",
    "        return [h,b]\n",
    "    \n",
    "    \n",
    "elif mode=='node':   \n",
    "    # Weight sparsity control with Hoyer's sparsness (Node wise)\n",
    "    def Hoyers_sparsity_control(w_,b_vec,max_b,tg):\n",
    "    \n",
    "        # Get value of weight\n",
    "        W=sess.run(w_)\n",
    "        [nodes,dim]=W.shape\n",
    "        \n",
    "        # Calculate L1 and L2 norm \n",
    "        L1=LA.norm(W,1,axis=0)\n",
    "        L2=LA.norm(W,2,axis=0)\n",
    "        \n",
    "        h_vec = np.zeros((1,dim))\n",
    "        tg_vec = np.ones(dim)*tg\n",
    "        \n",
    "        # Calculate hoyer's sparsness\n",
    "        h_vec=(np.sqrt(nodes)-(L1/L2))/(np.sqrt(nodes)-1)\n",
    "        \n",
    "        # Update beta\n",
    "        b_vec-=lr_beta*np.sign(h_vec-tg_vec)\n",
    "        \n",
    "        # Trim value\n",
    "        b_vec[b_vec<0.0]=0.0\n",
    "        b_vec[b_vec>max_b]=max_b\n",
    "        \n",
    "               \n",
    "        return [h_vec,b_vec]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create them by using those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = lr_init\n",
    "\n",
    "Beta_vec = build_betavec()\n",
    "\n",
    "L1_loss = build_L1loss()\n",
    "L2_loss = [tf.reduce_sum(tf.square(w[i])) for i in np.arange(np.shape(nodes)[0]-1)] \n",
    "\n",
    "cost = build_cost()\n",
    "\n",
    "\n",
    "# Make learning rate as placeholder to update learning rate every iterarion \n",
    "Lr=tf.placeholder(tf.float32)\n",
    "optimizer=build_optimizer(Lr)\n",
    "  \n",
    "\n",
    "if not autoencoder:\n",
    "    correct_prediction=tf.equal(tf.argmax(output_layer,1),tf.argmax(Y,1))  \n",
    "        \n",
    "    # calculate mean error(accuracy) depending on the frequency it predicts correctly   \n",
    "    error=1-tf.reduce_mean(tf.cast(correct_prediction,tf.float32))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Condition check part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check conditions to catch some possible errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "condition=False\n",
    "\n",
    "print()\n",
    "\n",
    "if np.shape(nodes)[0] <3:\n",
    "    print(\"Error : Not enough hidden layer number.\")\n",
    "elif (autoencoder==False) & (np.shape(train_input)[0] != np.shape(train_output)[0]):\n",
    "    print(\"Error : The sizes of input train dataset and output train dataset don't match. \")  \n",
    "elif (autoencoder==False) & (np.shape(test_input)[0] != np.shape(test_output)[0]):\n",
    "    print(\"Error : The sizes of input test dataset and output test dataset don't match. \")     \n",
    "elif not ((mode=='layer') | (mode=='node')):\n",
    "    print(\"Error : Select a valid mode. \") \n",
    "elif (np.any(np.array(tg_hsp)<0)) | (np.any(np.array(tg_hsp)>1)):  \n",
    "    print(\"Error : Please set the target sparsities appropriately.\")\n",
    "elif autoencoder!=True & autoencoder!=False:\n",
    "    print(\"Error : Please set the autoencoder mode appropriately.\")\n",
    "else:\n",
    "    condition=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Training & test part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If condition is met, start session where training and tesing would be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if condition==True:\n",
    "    \n",
    "    # make initializer        \n",
    "    init = tf.global_variables_initializer()              \n",
    "\n",
    "    \n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "        \n",
    "        # run tensorflow variable initialization\n",
    "        sess.run(init)\n",
    "    \n",
    "        # initialization    \n",
    "        def initialization():           \n",
    "            if mode=='layer': \n",
    "                beta=np.zeros(np.shape(nodes)[0]-2)\n",
    "                beta_vec = np.zeros(np.shape(nodes)[0]-2)\n",
    "                hsp = np.zeros(np.shape(nodes)[0]-2)            \n",
    "                plot_beta = np.zeros(np.shape(nodes)[0]-2)\n",
    "                plot_hsp = np.zeros(np.shape(nodes)[0]-2)\n",
    "                           \n",
    "            elif mode=='node':                       \n",
    "                beta = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]  \n",
    "                beta_vec=np.zeros(np.sum(nodes[1:-1]))\n",
    "                hsp = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]            \n",
    "                plot_beta = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_hsp = [np.zeros(nodes[i+1]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                \n",
    "            # make arrays to plot results\n",
    "            plot_lr=np.zeros(1)\n",
    "            plot_cost=np.zeros(1)\n",
    "            plot_train_err=np.zeros(1)\n",
    "            plot_test_err=np.zeros(1)\n",
    "            \n",
    "            return beta, beta_vec, hsp, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err\n",
    "                \n",
    "        beta, beta_vec, hsp, plot_beta, plot_hsp, plot_lr, plot_cost, plot_train_err, plot_test_err = initialization()\n",
    "        \n",
    "               \n",
    "\n",
    "        \n",
    "           \n",
    "        # Calculate how many mini-batch iterations\n",
    "        total_batch=int(np.shape(train_input)[0]/batch_size) \n",
    "               \n",
    "        # train and get cost\n",
    "        cost_avg=0.0\n",
    "        for epoch in np.arange(total_epoch):            \n",
    "            cost_epoch=0.0\n",
    "            \n",
    "            # Begin Annealing\n",
    "            if beginAnneal == 0:\n",
    "                lr = lr * 1.0\n",
    "            elif epoch+1 > beginAnneal:\n",
    "                lr = max( min_lr, (-decay_rate*(epoch+1) + (1+decay_rate*beginAnneal)) * lr )  \n",
    "            \n",
    "            \n",
    "            # Train at each mini batch    \n",
    "            for batch in np.arange(total_batch):\n",
    "                batch_x = train_input[batch*batch_size:(batch+1)*batch_size]\n",
    "                batch_y = train_output[batch*batch_size:(batch+1)*batch_size]\n",
    "                \n",
    "                # Get cost and optimize the model\n",
    "                if autoencoder:\n",
    "                    cost_batch,_=sess.run([cost,optimizer],feed_dict={Lr:lr, X:batch_x, Beta_vec:beta_vec })\n",
    "                else:                   \n",
    "                    cost_batch,_=sess.run([cost,optimizer],feed_dict={Lr:lr, X:batch_x, Y:batch_y, Beta_vec:beta_vec })\n",
    "                    \n",
    "                cost_epoch+=cost_batch/total_batch      \n",
    "        \n",
    "                \n",
    "                \n",
    "        \n",
    "                # run weight sparsity control function\n",
    "                for i in np.arange(np.shape(nodes)[0]-2):\n",
    "                    [hsp[i],beta[i]]=Hoyers_sparsity_control(w[i], beta[i], max_beta[i], tg_hsp[i])   \n",
    "                    \n",
    "                if mode=='layer':               \n",
    "                    beta_vec=beta                      \n",
    "                elif mode=='node':                              \n",
    "                    beta_vec=[item for sublist in beta for item in sublist]\n",
    "                    \n",
    "            train_err_epoch=sess.run(error,feed_dict={X:train_input, Y:train_output})\n",
    "            plot_train_err=np.hstack([plot_train_err,[train_err_epoch]])\n",
    "            \n",
    "            test_err_epoch=sess.run(error,feed_dict={X:test_input, Y:test_output})\n",
    "            plot_test_err=np.hstack([plot_test_err,[test_err_epoch]])\n",
    "            \n",
    "            \n",
    "            # make space to plot beta, sparsity level\n",
    "            plot_lr=np.hstack([plot_lr,[lr]])\n",
    "            plot_cost=np.hstack([plot_cost,[cost_epoch]])\n",
    "\n",
    "            \n",
    "            # save footprint for plot\n",
    "            if mode=='layer':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[hsp[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[beta_vec[i]]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                \n",
    "            elif mode=='node':\n",
    "                plot_hsp=[np.vstack([plot_hsp[i],[np.transpose(hsp[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "                plot_beta=[np.vstack([plot_beta[i],[np.transpose(beta[i])]]) for i in np.arange(np.shape(nodes)[0]-2)]\n",
    "\n",
    "            \n",
    "                    \n",
    "            # Print cost at each epoch        \n",
    "            print(\"< Epoch\", \"{:02d}\".format(epoch+1),\"> Cost : \", \"{:.4f}\".format(cost_epoch))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # Print final accuracy of test set\n",
    "        if not autoencoder:\n",
    "            print(\"Accuracy :\",1-test_err_epoch)\n",
    "            \n",
    "else:\n",
    "    # Don't run the sesstion but print 'failed' if any condition is unmet\n",
    "    print(\"Failed!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Plot & save results part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, show the the results and save them as .mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if condition==True:\n",
    "       \n",
    "    # Plot the change of learning rate\n",
    "    plt.title(\"Learning rate plot\",fontsize=16)\n",
    "    plot_lr=plot_lr[1:]\n",
    "    plt.ylim(0.0, lr_init*1.2)\n",
    "    plt.plot(plot_lr)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the change of cost\n",
    "    plt.title(\"Cost plot\",fontsize=16)\n",
    "    plot_cost=plot_cost[1:]\n",
    "    plt.plot(plot_cost)\n",
    "    plt.yscale('log')\n",
    "    plt.show()     \n",
    "    \n",
    "    \n",
    "\n",
    "    # Plot test error\n",
    "    plt.title(\"Training & Test error\",fontsize=16)\n",
    "    plot_test_err=plot_test_err[1:]\n",
    "    plt.plot(plot_train_err)\n",
    "    plt.hold\n",
    "    plt.plot(plot_test_err)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.legend(['Training error', 'Test error'],loc='upper right')\n",
    "#    plt.yscale('log')\n",
    "    plt.show() \n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    # Plot the change of beta value\n",
    "    print(\"\")       \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        print(\"                  < Hidden layer\",i+1,\">\")\n",
    "        plt.title(\"Beta plot\",fontsize=16)\n",
    "        plot_beta[i]=plot_beta[i][1:]\n",
    "        plt.plot(plot_beta[i])\n",
    "        plt.ylim(0.0, np.max(max_beta)*1.2)\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot the change of Hoyer's sparsness\n",
    "    print(\"\")            \n",
    "    for i in np.arange(np.shape(nodes)[0]-2):\n",
    "        print(\"\")\n",
    "        print(\"                  < Hidden layer\",i+1,\">\")\n",
    "        plt.title(\"Hoyer's sparsness plot\",fontsize=16)\n",
    "        plot_hsp[i]=plot_hsp[i][1:]\n",
    "        plt.plot(plot_hsp[i])\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        plt.show()\n",
    "    \n",
    "    # make a new 'results' directory in the current directory\n",
    "    current_directory = os.getcwd()\n",
    "    final_directory = os.path.join(current_directory, r'results')\n",
    "    if not os.path.exists(final_directory):\n",
    "        os.makedirs(final_directory) \n",
    "        \n",
    "    # save results as .mat file\n",
    "    scipy.io.savemat(\"results/result_learningrate.mat\", mdict={'lr': plot_lr})\n",
    "    scipy.io.savemat(\"results/result_cost.mat\", mdict={'cost': plot_cost})\n",
    "    scipy.io.savemat(\"results/result_beta.mat\", mdict={'beta': plot_beta})\n",
    "    scipy.io.savemat(\"results/result_hsp.mat\", mdict={'hsp': plot_hsp})\n",
    "\n",
    "else:\n",
    "    None "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
